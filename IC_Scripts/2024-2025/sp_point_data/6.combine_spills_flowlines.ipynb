{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2052bb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate coordinates between datasets: 73\n",
      "Flowline points before removal: 369782\n",
      "Flowline points after removal:  369709\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "os.chdir('/Users/ichittumuri/Desktop/MINES/COGCC-Risk-Analysis/Data')\n",
    "\n",
    "# --- Load datasets ---\n",
    "flowlines_pts = gpd.read_file(\"flowline_points_50m_dedup.geojson\")\n",
    "spills_pts = gpd.read_file(\"spills_w_flowline_attributes.geojson\")\n",
    "\n",
    "# Ensure both are in the same CRS\n",
    "if flowlines_pts.crs != spills_pts.crs:\n",
    "    spills_pts = spills_pts.to_crs(flowlines_pts.crs)\n",
    "\n",
    "# --- Coordinate keys for matching (rounded to avoid float noise) ---\n",
    "flowlines_pts[\"coord_key\"] = flowlines_pts.geometry.apply(lambda p: (round(p.x, 6), round(p.y, 6)))\n",
    "spills_pts[\"coord_key\"] = spills_pts.geometry.apply(lambda p: (round(p.x, 6), round(p.y, 6)))\n",
    "\n",
    "# --- Find intersection of coordinate keys ---\n",
    "common_coords = set(flowlines_pts[\"coord_key\"]) & set(spills_pts[\"coord_key\"])\n",
    "print(f\"Number of duplicate coordinates between datasets: {len(common_coords)}\")\n",
    "\n",
    "# --- Remove duplicates from flowlines, keep spills intact ---\n",
    "flowlines_no_dups = flowlines_pts[~flowlines_pts[\"coord_key\"].isin(common_coords)].copy()\n",
    "\n",
    "print(f\"Flowline points before removal: {len(flowlines_pts)}\")\n",
    "print(f\"Flowline points after removal:  {len(flowlines_no_dups)}\")\n",
    "\n",
    "# --- Optional: save ---\n",
    "# flowlines_no_dups.to_file(\"flowline_points_50m_dedup_nospilldups.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5728c2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NA and Zero Counts: combined ---\n",
      "                   column  na_count  zero_count\n",
      "0               unique_id         0           0\n",
      "1         operator_number         0           0\n",
      "2             flowline_id         0           0\n",
      "3             location_id         0           0\n",
      "4             diameter_in         0          56\n",
      "5               length_ft         0           0\n",
      "6  max_operating_pressure         0          84\n",
      "7             line_age_yr         0           0\n",
      "8                    risk         0      369782\n",
      "9           geod_length_m         0           0\n",
      "\n",
      "--- NA and Zero Counts: final_subset ---\n",
      "                   column  na_count  zero_count\n",
      "0               unique_id         0           0\n",
      "1         operator_number         0           0\n",
      "2             flowline_id         0           0\n",
      "3             location_id         0           0\n",
      "4             diameter_in         0           0\n",
      "5               length_ft         0           0\n",
      "6  max_operating_pressure         0           0\n",
      "7             line_age_yr         0           0\n",
      "8        match_distance_m         0           0\n",
      "9                    risk         0           0\n"
     ]
    }
   ],
   "source": [
    "def check_na_zero(df):\n",
    "    results = []\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            na_count = df[col].isna().sum()\n",
    "            zero_count = (df[col] == 0).sum()\n",
    "            results.append({\n",
    "                'column': col,\n",
    "                'na_count': na_count,\n",
    "                'zero_count': zero_count\n",
    "            })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n--- NA and Zero Counts: combined ---\")\n",
    "print(check_na_zero(flowlines_pts))\n",
    "\n",
    "print(\"\\n--- NA and Zero Counts: final_subset ---\")\n",
    "print(check_na_zero(spills_pts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44af2c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(370189, 22)\n",
      "['unique_id', 'operator_name', 'operator_number', 'flowline_id', 'location_id', 'status', 'flowline_action', 'location_type', 'fluid', 'material', 'diameter_in', 'length_ft', 'max_operating_pressure', 'line_age_yr', 'construct_date', 'risk', 'geod_length_m', 'geometry', 'coord_key', 'match_distance_m', 'incident_date', 'root_cause']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Ensure both have the same CRS before combining\n",
    "if flowlines_no_dups.crs != spills_pts.crs:\n",
    "    spills_pts = spills_pts.to_crs(flowlines_no_dups.crs)\n",
    "\n",
    "# Row-bind (outer join keeps all columns from both datasets)\n",
    "combined = gpd.GeoDataFrame(\n",
    "    pd.concat([flowlines_no_dups, spills_pts], ignore_index=True),\n",
    "    geometry=\"geometry\",\n",
    "    crs=flowlines_no_dups.crs\n",
    ")\n",
    "\n",
    "print(combined.shape)\n",
    "print(combined.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86b497bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final combined dataset size: 370189\n",
      "Unique coordinate locations: 370189\n",
      "['unique_id', 'operator_name', 'operator_number', 'flowline_id', 'location_id', 'status', 'flowline_action', 'location_type', 'fluid', 'material', 'diameter_in', 'length_ft', 'max_operating_pressure', 'line_age_yr', 'construct_date', 'risk', 'geod_length_m', 'geometry', 'coord_key', 'match_distance_m', 'incident_date', 'root_cause']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final combined dataset size: {len(combined)}\")\n",
    "print(f\"Unique coordinate locations: {combined.geometry.apply(lambda p: (round(p.x, 6), round(p.y, 6))).nunique()}\")\n",
    "\n",
    "# Optional: save to file\n",
    "# combined.to_file(\"flowlines_and_spills_combined.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8989c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d4a8418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unique_id', 'operator_name', 'operator_number', 'flowline_id', 'location_id', 'status', 'flowline_action', 'location_type', 'fluid', 'material', 'diameter_in', 'length_ft', 'max_operating_pressure', 'line_age_yr', 'avg_elevation', 'construct_date', 'incident_date', 'geod_length_m', 'match_distance_m', 'root_cause', 'risk', 'coord_key', 'geometry', 'avg_population']\n"
     ]
    }
   ],
   "source": [
    "print(combined.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8f16eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 370189\n",
      "Number of columns: 23\n",
      "Rows with risk = 1: 480\n",
      "Rows with risk = 0: 369709\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f941b4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37df2bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio import features\n",
    "from pyproj import CRS\n",
    "\n",
    "dem = rasterio.open('output_USGS30m.tif')\n",
    "dem_crs = CRS(dem.crs)\n",
    "if combined.crs != dem_crs:\n",
    "    combined = combined.to_crs(dem_crs)\n",
    "\n",
    "def get_elevation(point, dem):\n",
    "    try:\n",
    "        val = list(dem.sample([(point.x, point.y)]))[0][0]\n",
    "        if dem.nodata is not None and val == dem.nodata:\n",
    "            return None\n",
    "        return val\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "combined['avg_elevation'] = combined.geometry.apply(lambda pt: get_elevation(pt, dem))\n",
    "combined = combined.drop(columns=['index_right'], errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46935033",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 27. Load additional datasets and join population density and elevation\n",
    "pop_density = gpd.read_file('Population_Density_(Census_Tracts)').to_crs(combined.crs)\n",
    "\n",
    "print('Summary of Census Tract Data:')\n",
    "print(pop_density.info())\n",
    "print('\\nFirst few rows of the data:')\n",
    "print(pop_density.head())\n",
    "\n",
    "joined = gpd.sjoin(combined, pop_density, how='left', predicate='within')\n",
    "combined['avg_population'] = joined['Populati_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd28bc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired = [\n",
    "    'unique_id', 'operator_name', 'operator_number', 'flowline_id', 'location_id',\n",
    "    'status', 'flowline_action', 'location_type', 'fluid', 'material',\n",
    "    'diameter_in', 'length_ft', 'max_operating_pressure', 'line_age_yr', 'avg_elevation', 'avg_population',\n",
    "    'construct_date', 'incident_date', 'geod_length_m', 'match_distance_m',\n",
    "    'root_cause', 'risk', 'coord_key', 'geometry'\n",
    "]\n",
    "\n",
    "# keep desired (that actually exist), then append any remaining columns\n",
    "ordered_cols = [c for c in desired if c in combined.columns] + \\\n",
    "               [c for c in combined.columns if c not in desired]\n",
    "\n",
    "combined = combined[ordered_cols]\n",
    "\n",
    "# re-affirm geometry column (good practice after reindexing)\n",
    "combined = combined.set_geometry('geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bff2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined.to_file(\"full_final_dataset.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c5f825",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows: {len(combined)}\")\n",
    "print(f\"Number of columns: {combined.shape[1]}\")\n",
    "print(f\"Rows with risk = 1: {(combined['risk'] == 1).sum()}\")\n",
    "print(f\"Rows with risk = 0: {(combined['risk'] == 0).sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecmc_env (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
