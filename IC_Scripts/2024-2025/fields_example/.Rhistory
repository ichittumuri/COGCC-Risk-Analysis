head( pvc)
# each combination of resin/operator has two replications
table( pvc$resin, pvc$operator)
fullFit<- lm(psize ~ operator*resin, pvc )
ANOVATable<- anova( fullFit)
ANOVATable
# the complete sums of squares subtracting off the grand mean is
SSTotal<- sum( (pvc$psize - mean(pvc$psize))^2)
SSTotal
# compare to
sum( ANOVATable[,2])
# exploit the fact we have the factor levels coded as categorical
mu<- mean( pvc$psize)
resinMeans<- tapply( pvc$psize,  pvc$resin, mean)
alpha<- resinMeans - mu
print( mu)
print( alpha)
N<- nrow( pvc)
I<- 8
J<- 3
K <- 2
# note that below the operator and replicates are treated as
#repeated measurements on the railcar factor.
SSNumerator<-   (J*K)*sum( ( alpha)^2 )
SSResiduals  <- sum( (pvc$psize - resinMeans[pvc$resin])^2)
MSNum<-   SSNumerator/ (I-1)
MSDen<-  SSResiduals/ ( N-8)
myFStat<- MSNum/MSDen
print( myFStat)
# compare to
fit1<- lm( psize ~ resin, pvc)
anova(fit1)
muij<- tapply(  pvc$psize, pvc[,c("resin","operator")], mean)
mu<- mean( muij)
n<- nrow( pvc)
I<- nrow(muij)
J<- ncol( muij)
K<- nrow( pvc)/ (I*J)
alpha<- rowMeans(muij) - mu
beta<-  colMeans(muij) - mu
MSAlpha <- K*(J*sum(alpha^2))/(I-1)
print( MSAlpha)
MSBeta<- K*(I*sum( beta^2))/(J-1)
resid<-  pvc$psize - mu - alpha[pvc$resin] - beta[pvc$operator]
sigma2<-  sum( resid^2)/ (n - (I+J-1))
print( c(MSAlpha, MSBeta, sigma2))
print( c(MSAlpha, MSBeta)/sigma2)
### here an alternative computation using the sums of sqaures
### for constrained vs unconstrained residuals
##
resid<-  pvc$psize - mu - alpha[pvc$resin] - beta[pvc$operator]
residAlpha<-  pvc$psize - mu - alpha[pvc$resin]
MSAplha1<-  sum(residAlpha^2)-  sum( resid^2)/ (8-1)
print( MSAlpha1)
residAlpha<-  pvc$psize - mu - alpha[pvc$resin]
MSAplha1<-  (sum(residAlpha^2)-  sum( resid^2))/ (8-1)
MSAlpha1
MSAlpha1<-  (sum(residAlpha^2)-  sum( resid^2))/ (8-1)
print( MSAlpha1)
print( c(MSAlpha, MSBeta, sigma2))
MSBeta1<-  sum(residBeta^2)-  sum( resid^2)/ (3-1)
residBeta<-  pvc$psize - mu - beta[pvc$resin]
MSBeta1<-  sum(residBeta^2)-  sum( resid^2)/ (3-1)
print( MSBeta1)
beta[pvc$resin]
residBeta<-  pvc$psize - mu - beta[pvc$operator]
MSBeta1<-  sum(residBeta^2)-  sum( resid^2)/ (3-1)
print( MSBeta1)
MSBeta1<-  (sum(residBeta^2)-  sum( resid^2))/ (3-1)
MSBeta1
sigma2
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(library( fields))
setwd("~/Dropbox/Home/Teaching/MATH531/MATH-531/MATH531S2024/Homework")
load("pvc.rda")
head( pvc)
# each combination of resin/operator has two replications
table( pvc$resin, pvc$operator)
fullFit<- lm(psize ~ operator*resin, pvc )
ANOVATable<- anova( fullFit)
ANOVATable
# the complete sums of squares subtracting off the grand mean is
SSTotal<- sum( (pvc$psize - mean(pvc$psize))^2)
SSTotal
# compare to
sum( ANOVATable[,2])
# exploit the fact we have the factor levels coded as categorical
mu<- mean( pvc$psize)
resinMeans<- tapply( pvc$psize,  pvc$resin, mean)
alpha<- resinMeans - mu
print( mu)
print( alpha)
N<- nrow( pvc)
I<- 8
J<- 3
K <- 2
# note that below the operator and replicates are treated as
#repeated measurements on the railcar factor.
SSNumerator<-   (J*K)*sum( ( alpha)^2 )
SSResiduals  <- sum( (pvc$psize - resinMeans[pvc$resin])^2)
MSNum<-   SSNumerator/ (I-1)
MSDen<-  SSResiduals/ ( N-8)
myFStat<- MSNum/MSDen
print( myFStat)
# compare to
fit1<- lm( psize ~ resin, pvc)
anova(fit1)
muij<- tapply(  pvc$psize, pvc[,c("resin","operator")], mean)
mu<- mean( muij)
n<- nrow( pvc)
I<- nrow(muij)
J<- ncol( muij)
K<- nrow( pvc)/ (I*J)
alpha<- rowMeans(muij) - mu
beta<-  colMeans(muij) - mu
MSAlpha <- K*(J*sum(alpha^2))/(I-1)
print( MSAlpha)
MSBeta<- K*(I*sum( beta^2))/(J-1)
resid<-  pvc$psize - mu - alpha[pvc$resin] - beta[pvc$operator]
sigma2<-  sum( resid^2)/ (n - (I+J-1))
print( c(MSAlpha, MSBeta, sigma2))
print( c(MSAlpha, MSBeta)/sigma2)
### here an alternative computation using the sums of sqaures
### for constrained vs unconstrained residuals
##
resid<-  pvc$psize - mu - alpha[pvc$resin] - beta[pvc$operator]
residAlpha<-  pvc$psize - mu - alpha[pvc$resin]
MSAlpha1<-  (sum(residAlpha^2)-  sum( resid^2))/ (8-1)
print( MSAlpha1)
residBeta<-  pvc$psize - mu - beta[pvc$operator]
MSBeta1<-  (sum(residBeta^2)-  sum( resid^2))/ (3-1)
print( MSBeta1)
##########
#### checking
mainFit<- lm(psize ~ operator + resin, pvc )
print( anova( mainFit))
fitNoInteraction<- lm( psize ~ resin + operator, pvc)
anova(fitNoInteraction)
muijHat<- tapply( pvc$psize, list(pvc$resin, pvc$operator ), mean)
alphaHat <- rowMeans( muijHat) - mean( pvc$psize)
betaHat<-   colMeans(  muijHat)  - mean( pvc$psize)
indexResin<-as.numeric( pvc$resin)
indexOperator<-as.numeric( pvc$operator)
vecGrand<- rep( mean( pvc$psize), 48)
vecMeans<- muijHat[ cbind(indexResin, indexOperator)]
vecAlpha<- alphaHat[indexResin]
vecBeta<-  betaHat[indexOperator]
vecGamma<- vecMeans -  (vecGrand + vecAlpha + vecBeta)
residual<- pvc$psize - vecMeans
tablePVC<- cbind(
pvc$psize,
vecGrand,
vecAlpha,
vecBeta,
vecGamma,
residual
)
print( round(tablePVC,2))
round(colSums( tablePVC^2),3)
# compare to  column 2 sums of squares below
anova( fullFit)
sum( alpha[pvc$resin]^2)/(8-1)
(sum(residAlpha^2)-  sum( resid^2))/ (8-1)
yHatAlpha<-  mu + alpha[pvc$resin]
yHatFull<- mu + alpha[pvc$resin] + beta[pvc$operator]
MSAlpha1<-  (sum((yHatAlpha - yHatFull )^2)/ (8-1)
)
MSAlpha1<-  (sum((yHatAlpha - yHatFull )^2))/ (8-1)
MSAlpha1
resid<-  pvc$psize - mu - alpha[pvc$resin] - beta[pvc$operator]
yHatAlphaH<-  mu + beta[pvc$operator] # alphas set to zero here
yHatFull<- mu + alpha[pvc$resin] + beta[pvc$operator]
MSAlpha1<-  (sum((yHatAlpha - yHatFull )^2))/ (8-1)
MSAlpha1
resid<-  pvc$psize - mu - alpha[pvc$resin] - beta[pvc$operator]
yHatAlphaH<-  mu + beta[pvc$operator] # alphas set to zero here
yHatFull<- mu + alpha[pvc$resin] + beta[pvc$operator]
MSAlpha1<-  (sum((yHatAlphaH - yHatFull )^2))/ (8-1)
print( MSAlpha1)
MSAlpha2<-  (sum((y-yHatAlphaH)^2) -sum((y-yHatFull)^2) )/ (8-1)
y
y<- pvc$psize
yHatAlphaH<-  mu + beta[pvc$operator] # alphas set to zero here
yHatFull<- mu + alpha[pvc$resin] + beta[pvc$operator]
# sums of squares of differences of predicted values
MSAlpha1<-  (sum((yHatAlphaH - yHatFull )^2))/ (8-1)
print( MSAlpha1)
### also via residuals
# difference in sums  squares of residuals
MSAlpha2<-  (sum((y-yHatAlphaH)^2) -sum((y-yHatFull)^2) )/ (8-1)
##########
####  and checking
mainFit<- lm(psize ~ operator + resin, pvc )
print( anova( mainFit))
MSAlpha2
1/.27
setwd("~/Dropbox/Home/Desktop2/PhDProjects/FlowIs")
source("~/Dropbox/Home/Desktop2/PhDProjects/FlowIs/testLogisticSmoother2.R")
source("~/Dropbox/Home/Desktop2/PhDProjects/FlowIs/testLogisticSmoother2.R")
set.seed(226)
n<- 50
s<-  sort(runif( n))
s<- seq( 0,1,length.out=n)
p<-  s*(1-s)^3 +.001
p<- p / (max(p)*1.05)
s<- cbind( s)
y<- rbinom( n,1, p )
sigma<- 1.5
set.seed(226)
n<- 50
s<-  sort(runif( n))
s<- seq( 0,1,length.out=n)
p<-  s*(1-s)^3 +.001
p<- p / (max(p)*1.05)
s<- cbind( s)
y<- rbinom( n,1, p )
L<- 25
sigmaGrid<- 10^seq( log10(.5), log10(15), length.out=L)
logLikeGrid<- rep( NA, L)
pOld<- mean(y)
nuStart<-  rep( log(pOld/(1-pOld)), length(y))
for( j in L:1){
fit<- logisticSmootherBayes(s, y,
nuStart=nuStart,
sigma=sigmaGrid[j],
aRange=2)
logLikeGrid[j]<- fit$logLike
nuStart<- fit$fitted.values
}
ind<- which.max(logLikeGrid)
fit<- logisticSmootherBayes(s, y,
nuStart=nuStart,
sigma=sigmaGrid[ind],
aRange=2)
plot( s, y)
lines(s,fit$pHat,
col="grey",
lwd=3, lty=2 )
print( sigmaGrid[ind])
sigma<-7.3
nuStart<- NULL
obj0<- logisticSmootherBayes(s, y, sigma=sigma,
nuStart=nuStart, aRange=NA)
n<- length( y)
X<- cbind( 1, s)
#K<- sigma^2*stationary.cov(s,s, Covariance = Tps.cov,
#                          lambda= 1/sigma^2)
K<- (sigma^2)*Tps.cov( s,s, cardinalX = c(.25, .75), m=2)
KC<- chol( K)
logDetK<- 2*sum( log( diag(KC)))
prod( diag( KC))
determinant( K)
logDetK<- 2*sum( log( diag(KC)))
logDetK
E<- matrix( rnorm(n*M ),n,M)
gSim<- t( KC)%*%E
M<- 10
E<- matrix( rnorm(n*M ),n,M)
gSim<- t( KC)%*%E
plot( gSim[,1])
plot( gSim[,2])
plot( gSim[,3])
plot( gSim[,4])
n<- length( y)
X<- cbind( 1, s)
#K<- sigma^2*stationary.cov(s,s, Covariance = Tps.cov,
#                          lambda= 1/sigma^2)
K<- (sigma^2)*Tps.cov( s,s, cardinalX = c(.25, .75), m=2)
KC<- chol( K)
logDetK<- 2*sum( log( diag(KC)))
E<- matrix( rnorm(n*M ),n,M)
gSim<- t( KC)%*%E
# find log likelihoods with simulated g
nuSim<- X%*%obj0$beta + gSim
dim(X %*% obj0$beta )
dim( gSim)
nuSim<- c(X%*%obj0$beta) + gSim
tmp<- log( 1 + exp( nuSim))
dim( tmp)
colSums( tmp)
logL<- t(nuSim)%*%y - colSums( tmp) - .5* colSums( E^2) - logDetK
logL
matplot( gSim, type="l")
setwd("~/Dropbox/Home/Desktop2/PhDProjects/FlowIs")
library( fields)
source("logisticSmoother.R")
source("logisticSmootherBayes.R")
n<- 500
set.seed(226)
s<-  sort(runif( n))
s<- seq( 0,1,length.out=n)
p<-  s*(1-s)^3 +.001
p<- p / (max(p)*1.05)
s<- cbind( s)
y<- rbinom( n,1, p )
plot( s, y, cex=.5, type="p", pch=16)
look0<- logisticSmootherBayes(s, y, sigma=sqrt(1/4e-3), aRange=2)
set.seed(226)
n<- 50
s<-  sort(runif( n))
s<- seq( 0,1,length.out=n)
p<-  s*(1-s)^3 +.001
p<- p / (max(p)*1.05)
s<- cbind( s)
y<- rbinom( n,1, p )
L<- 25
sigmaGrid<- 10^seq( log10(.5), log10(15), length.out=L)
logLikeGrid<- rep( NA, L)
pOld<- mean(y)
nuStart<-  rep( log(pOld/(1-pOld)), length(y))
for( j in L:1){
fit<- logisticSmootherBayes(s, y,
nuStart=nuStart,
sigma=sigmaGrid[j],
aRange=2)
logLikeGrid[j]<- fit$logLike
nuStart<- fit$fitted.values
}
ind<- which.max(logLikeGrid)
print( sigmaGrid[ind])
fit<- logisticSmootherBayes(s, y,
nuStart=nuStart,
sigma=sigmaGrid[ind],
aRange=2)
plot( s, y)
lines(s,fit$pHat,
col="grey",
lwd=3, lty=2 )
print( sigmaGrid[ind])
set.seed(226)
n<- 50
s<-  sort(runif( n))
s<- seq( 0,1,length.out=n)
p<-  s*(1-s)^3 +.001
p<- p / (max(p)*1.05)
s<- cbind( s)
y<- rbinom( n,1, p )
L<- 25
sigmaGrid<- 10^seq( log10(.5), log10(15), length.out=L)
logLikeGrid<- rep( NA, L)
pOld<- mean(y)
nuStart<-  rep( log(pOld/(1-pOld)), length(y))
for( j in L:1){
fit<- logisticSmootherBayes(s, y,
nuStart=nuStart,
sigma=sigmaGrid[j])
logLikeGrid[j]<- fit$logLike
nuStart<- fit$fitted.values
}
ind<- which.max(logLikeGrid)
print( sigmaGrid[ind])
fit<- logisticSmootherBayes(s, y,
nuStart=nuStart,
sigma=sigmaGrid[ind])
plot( s, y)
lines(s,fit$pHat,
col="grey",
lwd=3, lty=2 )
source("~/Dropbox/Home/Desktop2/PhDProjects/FlowIs/testLogisticSmoother3.R")
source("~/Dropbox/Home/Desktop2/PhDProjects/FlowIs/testLogisticSmoother3.R")
plot( s, y, pch=16, col="grey")
lines(s,fit$pHat,
lwd=3, lty=2, col="magenta" )
lines(s,pTrue,col="orange3"  )
source("~/Dropbox/Home/Desktop2/PhDProjects/FlowIs/logisticSmootherBayes.R")
source("~/Dropbox/Home/Desktop2/PhDProjects/FlowIs/testLogisticSmoother3.R")
source("~/Dropbox/Home/Desktop2/PhDProjects/FlowIs/logisticSmootherBayes.R")
source("~/Dropbox/Home/Desktop2/PhDProjects/FlowIs/testLogisticSmoother3.R")
fit1<- logisticSmootherBayes(s, y,
nuStart=nuStart,
sigma=10)
plot( s, y, pch=16, col="grey")
lines(s,fit$pHat,
lwd=3, lty=2, col="magenta" )
lines(s,fit1$pHat,
lwd=3, lty=2, col="magenta" )
lines(s,pTrue,col="orange3"  )
fit1<- logisticSmootherBayes(s, y,
nuStart=nuStart,
sigma=30)
plot( s, y, pch=16, col="grey")
lines(s,fit$pHat,
lwd=3, lty=2, col="magenta" )
lines(s,fit1$pHat,
lwd=3, lty=2, col="magenta" )
lines(s,pTrue,col="orange3"  )
fit1<- logisticSmootherBayes(s, y,
nuStart=nuStart,
sigma=25)
plot( s, y, pch=16, col="grey")
lines(s,fit$pHat,
lwd=3, lty=2, col="magenta" )
lines(s,fit1$pHat,
lwd=3, lty=2, col="magenta" )
lines(s,pTrue,col="orange3"  )
y<- rbinom( n,1, pTrue )
L<- 25
sigmaGrid<- 10^seq( log10(.5), log10(15), length.out=L)
logLikeGrid<- rep( NA, L)
pOld<- mean(y)
nuStart<-  rep( log(pOld/(1-pOld)), length(y))
for( j in L:1){
fit<- logisticSmootherBayes(s, y,
nuStart=nuStart,
sigma=sigmaGrid[j])
logLikeGrid[j]<- fit$logLike
nuStart<- fit$fitted.values
}
ind<- which.max(logLikeGrid)
print( sigmaGrid[ind])
fit<- logisticSmootherBayes(s, y,
nuStart=nuStart,
sigma=sigmaGrid[ind])
fit1<- logisticSmootherBayes(s, y,
nuStart=nuStart,
sigma=25)
plot( s, y, pch=16, col="grey")
lines(s,fit$pHat,
lwd=3, lty=2, col="magenta" )
lines(s,fit1$pHat,
lwd=3, lty=2, col="magenta" )
lines(s,pTrue,col="orange3"  )
source("~/Dropbox/Home/Desktop2/PhDProjects/FlowIs/testLogisticSmoother3.R")
sigma=sigmaGrid[ind])
fit1<- logisticSmootherBayes(s, y,
nuStart=nuStart,
sigma=50)
plot( s, y, pch=16, col="grey")
lines(s,fit$pHat,
lwd=3, lty=2, col="magenta" )
lines(s,fit1$pHat,
lwd=3, lty=2, col="magenta" )
lines(s,pTrue,col="orange3"  )
fit1<- logisticSmootherBayes(s, y,
nuStart=nuStart,
sigma=40)
plot( s, y, pch=16, col="grey")
lines(s,fit$pHat,
lwd=3, lty=2, col="magenta" )
lines(s,fit1$pHat,
lwd=3, lty=2, col="magenta" )
lines(s,pTrue,col="orange3"  )
y<- rbinom( n,1, pTrue )
L<- 25
sigmaGrid<- 10^seq( log10(.5), log10(15), length.out=L)
logLikeGrid<- rep( NA, L)
pOld<- mean(y)
nuStart<-  rep( log(pOld/(1-pOld)), length(y))
for( j in L:1){
fit<- logisticSmootherBayes(s, y,
nuStart=nuStart,
sigma=sigmaGrid[j])
logLikeGrid[j]<- fit$logLike
nuStart<- fit$fitted.values
}
ind<- which.max(logLikeGrid)
print( sigmaGrid[ind])
fit<- logisticSmootherBayes(s, y,
nuStart=nuStart,
sigma=sigmaGrid[ind])
fit1<- logisticSmootherBayes(s, y,
nuStart=nuStart,
sigma=40)
plot( s, y, pch=16, col="grey")
lines(s,fit$pHat,
lwd=3, lty=2, col="magenta" )
lines(s,fit1$pHat,
lwd=3, lty=2, col="magenta" )
lines(s,pTrue,col="orange3"  )
aRange<- 2
sigma<- 10
obj0<- logisticSmootherBayes(s, y, sigma=sigma,
nuStart=nuStart, aRange=aRange)
n<- length( y)
X<- cbind( 1, s)
K<- sigma^2*stationary.cov(s,s, Covariance = Matern,
smoothness= 1.5,
aRange= aRange)
KC<- chol( K)
logDetK<- 2*sum( log( diag(KC)))
E<- matrix( rnorm(n*M ),n,M)
gSim<- t( KC)%*%E
matplot( gSim, type="l")
nuSim<- c(X%*%obj0$beta) + gSim
tmp<- log( 1 + exp( nuSim))
logL<- t(nuSim)%*%y - colSums( tmp) - .5* colSums( E^2) - logDetK
logL
E<- matrix( rnorm(n*M ),n,M)
gSim<- t( KC)%*%E
# find log likelihoods with simulated g
nuSim<- c(X%*%obj0$beta) + gSim
tmp<- log( 1 + exp( nuSim))
logL<- t(nuSim)%*%y - colSums( tmp) - .5* colSums( E^2) - logDetK
logL
E<- matrix( rnorm(n*M ),n,M)
gSim<- t( KC)%*%E
# find log likelihoods with simulated g
nuSim<- c(X%*%obj0$beta) + gSim
tmp<- log( 1 + exp( nuSim))
logL<- t(nuSim)%*%y - colSums( tmp) - .5* colSums( E^2) - logDetK
M<- 500
E<- matrix( rnorm(n*M ),n,M)
gSim<- t( KC)%*%E
# find log likelihoods with simulated g
nuSim<- c(X%*%obj0$beta) + gSim
tmp<- log( 1 + exp( nuSim))
logL<- t(nuSim)%*%y - colSums( tmp) - .5* colSums( E^2) - logDetK
stats( logL)
hist( logL)
hist( logL, nclass=30)
hist( logL- mean(logL), nclass=30)
hist( (logL- mean(logL))/ sd( logL), nclass=30)
