grand_mean
alpha
beta
# Define the number of levels for resin and operator, and the number of replications per combination.
I <- 8
J <- 3
# Pre-allocate vectors to store calculated means and main effects.
alpha <- rep(NA, I)
muIDot <- rep(NA, I)
beta <- rep(NA, J)
muDotJ <- rep(NA, J)
# Calculate the grand mean for psize.
grand_mean <- mean(pvc$psize)
# Compute means and main effects for each resin level.
for (i in 1:I) {
ind <- pvc$resin == i
muIDot[i] <- mean(pvc$psize[ind])
alpha[i] <- muIDot[i] - grand_mean
}
# Compute means and main effects for each operator level.
for (j in 1:J) {
ind <- pvc$operator == j
muDotJ[j] <- mean(pvc$psize[ind])
beta[j] <- muDotJ[j] - grand_mean
}
# Ensure that the sum of the main effects for alpha and beta are zero to meet the constraint.
alpha <- alpha - mean(alpha)
beta <- beta - mean(beta)
# Output results to check calculations and constraints.
cat("Sum of alpha:", sum(alpha), "\n")
cat("Sum of beta:", sum(beta), "\n")
cat("Grand mean:", grand_mean, "\n")
print(alpha)
print(beta)
F_stat_resin <- F_stat
F_stat_resin
new_muDotJ <- rep(NA, 48)
for (i in 1:48) {
operatorLevel <- pvc$operator[i]
new_muDotJ[i] <- muDotJ[operatorLevel]
}
RSS <- sum((pvc$psize - new_muDotJ)^2)
F_stat_operator <- (16*sum((muDotJ - grand_mean)^2) / (3-1)) / (RSS / (48-3))
F_stat_operator
ANOVATable
# Assume previous definitions and calculations for `muDotJ` and `grand_mean` are correct.
# Calculate the F-statistic for the resin main effects directly from a stored value (seems like a placeholder):
F_stat_resin <- F_stat
print(F_stat_resin)
# Calculate the F-statistic for the operator main effects:
new_muDotJ <- rep(NA, 48)
for (i in 1:48) {
operatorLevel <- pvc$operator[i]
new_muDotJ[i] <- muDotJ[operatorLevel]
}
RSS_operator <- sum((pvc$psize - new_muDotJ)^2)
F_stat_operator <- ((n-J) * sum((muDotJ - grand_mean)^2) / (J-1)) / (RSS_operator / (n-J))
print(F_stat_operator)
# Display the ANOVA table for the full model to compare:
print(ANOVATable)
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(library( fields))
setwd("~/Downloads")
load("pvc.rda")
head( pvc)
# each combination of resin/operator has two replications
table( pvc$resin, pvc$operator)
fullFit<- lm(psize ~ operator*resin, pvc )
ANOVATable<- anova( fullFit)
ANOVATable
# the complete sums of squares subtracting off the grand mean is
SSTotal<- sum( (pvc$psize - mean(pvc$psize))^2)
SSTotal
# compare to
sum( ANOVATable[,2])
stats(pvc)
grand_mean <- mean(pvc$psize)
resinMeans <- tapply(pvc$psize, pvc$resin, mean)
alphaVec <- resinMeans - grand_mean
sum(alphaVec)
c(grand_mean, alphaVec)
muVec <- tapply(pvc$psize, pvc$resin, mean)
resinfit <- lm(psize ~ resin, data=pvc)
resinfit_summary <- summary(resinfit)
print(resinfit_summary$coefficients[1, 1])
coefs <- resinfit_summary$coefficients
for (j in 2:8) {
if (j <= length(coefs[,'Estimate'])) {
model_estimated_mean <- coefs[1, 1] + coefs[j, 1]
manual_mean <- muVec[j]
print(c(model_estimated_mean, manual_mean))
}
}
n <- 48
I <- 8
K <- 6
new_muVec <- rep(NA, n)
for (i in 1:n) {
resinLevel <- pvc$resin[i]
new_muVec[i] <- muVec[resinLevel]
}
RSS <- sum((pvc$psize - new_muVec)^2)
F_stat <- ((sum((muVec - grand_mean)^2) * K) / (I-1)) / (RSS / (n-I))
print(F_stat)
anova(resinfit)
I <- 8
J <- 3
alpha <- rep(NA, I)
muIDot <- rep(NA, I)
beta <- rep(NA, J)
muDotJ <- rep(NA, J)
grand_mean <- mean(pvc$psize)
for (i in 1:I) {
ind <- pvc$resin == i
muIDot[i] <- mean(pvc$psize[ind])
alpha[i] <- muIDot[i] - grand_mean
}
for (j in 1:J) {
ind <- pvc$operator == j
muDotJ[j] <- mean(pvc$psize[ind])
beta[j] <- muDotJ[j] - grand_mean
}
alpha <- alpha - mean(alpha)
beta <- beta - mean(beta)
# Output results to check calculations and constraints.
cat("Sum of alpha:", sum(alpha), "\n")
cat("Sum of beta:", sum(beta), "\n")
cat("Grand mean:", grand_mean, "\n")
print(alpha)
print(beta)
F_stat_resin <- F_stat
print(F_stat_resin)
new_muDotJ <- rep(NA, 48)
for (i in 1:48) {
operatorLevel <- pvc$operator[i]
new_muDotJ[i] <- muDotJ[operatorLevel]
}
RSS_operator <- sum((pvc$psize - new_muDotJ)^2)
F_stat_operator <- ((n-J) * sum((muDotJ - grand_mean)^2) / (J-1)) / (RSS_operator / (n-J))
print(F_stat_operator)
print(ANOVATable)
p_value_resin <- pf(F_stat_resin, 7, 40, lower.tail = F)
p_value_operator <- pf(F_stat_operator, 7, 40, lower.tail = F)
print(p_value_resin)
print(p_value_operator)
pf(F_stat_resin, 8-1, 48-8, lower.tail = F)
pf(F_stat_operator, 8-1, 48-8, lower.tail = F)
pf(F_stat_resin, 8-1, 48-8, lower.tail = F)
pf(F_stat_operator, 8-1, 48-8, lower.tail = F)
p_value_resin <- pf(F_stat_resin, 7, 40, lower.tail = F)
p_value_operator <- pf(F_stat_operator, 7, 40, lower.tail = F)
print(p_value_resin)
print(p_value_operator)
p_value_resin <- pf(F_stat_resin, 7, 40, lower.tail = F)
p_value_operator <- pf(F_stat_operator, 7, 40, lower.tail = F)
print(p_value_resin)
print(p_value_operator)
fullFit <- lm(psize ~ operator*resin, pvc)
summary(fullFit)
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(library( fields))
setwd("~/Downloads")
load("pvc.rda")
head( pvc)
# each combination of resin/operator has two replications
table( pvc$resin, pvc$operator)
fullFit<- lm(psize ~ operator*resin, pvc )
ANOVATable<- anova( fullFit)
ANOVATable
# the complete sums of squares subtracting off the grand mean is
SSTotal<- sum( (pvc$psize - mean(pvc$psize))^2)
SSTotal
# compare to
sum( ANOVATable[,2])
stats(pvc)
grand_mean <- mean(pvc$psize)
resinMeans <- tapply(pvc$psize, pvc$resin, mean)
alphaVec <- resinMeans - grand_mean
sum(alphaVec)
c(grand_mean, alphaVec)
muVec <- tapply(pvc$psize, pvc$resin, mean)
resinfit <- lm(psize ~ resin, data=pvc)
resinfit_summary <- summary(resinfit)
print(resinfit_summary$coefficients[1, 1])
coefs <- resinfit_summary$coefficients
for (j in 2:8) {
if (j <= length(coefs[,'Estimate'])) {
model_estimated_mean <- coefs[1, 1] + coefs[j, 1]
manual_mean <- muVec[j]
print(c(model_estimated_mean, manual_mean))
}
}
n <- 48
I <- 8
K <- 6
new_muVec <- rep(NA, n)
for (i in 1:n) {
resinLevel <- pvc$resin[i]
new_muVec[i] <- muVec[resinLevel]
}
RSS <- sum((pvc$psize - new_muVec)^2)
F_stat <- ((sum((muVec - grand_mean)^2) * K) / (I-1)) / (RSS / (n-I))
print(F_stat)
anova(resinfit)
I <- 8
J <- 3
alpha <- rep(NA, I)
muIDot <- rep(NA, I)
beta <- rep(NA, J)
muDotJ <- rep(NA, J)
grand_mean <- mean(pvc$psize)
for (i in 1:I) {
ind <- pvc$resin == i
muIDot[i] <- mean(pvc$psize[ind])
alpha[i] <- muIDot[i] - grand_mean
}
for (j in 1:J) {
ind <- pvc$operator == j
muDotJ[j] <- mean(pvc$psize[ind])
beta[j] <- muDotJ[j] - grand_mean
}
alpha <- alpha - mean(alpha)
beta <- beta - mean(beta)
# Output results to check calculations and constraints.
cat("Sum of alpha:", sum(alpha), "\n")
cat("Sum of beta:", sum(beta), "\n")
cat("Grand mean:", grand_mean, "\n")
print(alpha)
print(beta)
F_stat_resin <- F_stat
print(F_stat_resin)
new_muDotJ <- rep(NA, 48)
for (i in 1:48) {
operatorLevel <- pvc$operator[i]
new_muDotJ[i] <- muDotJ[operatorLevel]
}
RSS_operator <- sum((pvc$psize - new_muDotJ)^2)
F_stat_operator <- ((n-J) * sum((muDotJ - grand_mean)^2) / (J-1)) / (RSS_operator / (n-J))
print(F_stat_operator)
print(ANOVATable)
p_value_resin <- pf(F_stat_resin, 7, 40, lower.tail = F)
p_value_operator <- pf(F_stat_operator, 7, 40, lower.tail = F)
print(p_value_resin)
print(p_value_operator)
fullFit <- lm(psize ~ operator*resin, pvc)
summary(fullFit)
indexResin <- as.numeric(pvc$resin)
indexOperator <- as.numeric(pvc$operator)
mu_ijHat <- matrix(tapply(pvc$psize, list(indexResin, indexOperator), mean), nrow=8, ncol=3)
vecMeans <- mu_ijHat[cbind(indexResin, indexOperator)]
vecGrand <- rep(mean(pvc$psize), 48)
alphaHat <- tapply(pvc$psize, pvc$resin, mean) - mean(pvc$psize)
betaHat <- tapply(pvc$psize, pvc$operator, mean) - mean(pvc$psize)
vecAlpha <- alphaHat[indexResin]
vecBeta <- betaHat[indexOperator]
vecGamma <- vecMeans - vecGrand - vecAlpha - vecBeta
residual <- pvc$psize - vecMeans
tablePVC <- data.frame(
psize = pvc$psize,
GrandMean = vecGrand,
ResinMainEffects = alphaHat[indexResin],
OperatorMainEffects = betaHat[indexOperator],
InteractionEffects = vecGamma,
Residuals = residual
)
print(tablePVC)
ANOVATable
sum(vecAlpha^2)
sum(vecBeta^2)
sum(vecGamma^2)
sum(residual^2)
# New cell= Cmd + Opt + I or alt + Cmd + I
# Run = Cmd + enter
# Comment out a whole chunk of code = Cmd + Shift + C
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_knit$set(root.dir = "/Users/ichittumuri/Desktop/MINES/COGCC-Risk-Analysis/Data")
# Load required libraries
library(sf)        # For spatial data handling
library(spBayes)   # For Bayesian spatial models
library(MASS)      # For generalized linear models
library(spdep)     # For spatial correlation structures
library(ggplot2)   # For visualization
library(dplyr)     # For data wrangling
library(fields)     # Spatial Kriging and thin-plate splines
library(leaflet)
library(scales)  # To rescale predictions between 0-1
df <- st_read("flowlines_pop_dems.geojson")
colSums(is.na(df))  # Check missing data per column
# Filter rows where root_cause is NA and count by risk values
table(df$risk[is.na(df$root_cause)])
unique(df$root_cause)
df$root_cause[is.na(df$root_cause)] <- "None"
str(df)
unique(df$risk)
df <- df %>% select(-Max_Elevation, -Min_Elevation)
# Duplicate data set
df_encoded <- df
# One-hot encoding
df_encoded$operator_number <- as.factor(df$operator_number)
df_encoded$flowline_id <- as.factor(df$flowline_id)
df_encoded$location_id <- as.factor(df$location_id)
df_encoded$status <- as.factor(df$status)
df_encoded$fluid <- as.factor(df$fluid)
df_encoded$material <- as.factor(df$material)
df_encoded$location_type <- as.factor(df$location_type)
df_encoded$root_cause <- as.factor(df$root_cause)
str(df_encoded)
colSums(is.na(df_encoded))  # Check missing data per column
library(leaflet)
leaflet(df_encoded) %>%
addProviderTiles(providers$CartoDB.Positron) %>%
addPolylines(color = ~ifelse(risk == 1, "red", "blue"), weight = 2, opacity = 0.7) %>%
addLegend("bottomright", colors = c("blue", "red"), labels = c("No Spill", "Spill"),
title = "Risk Level")
df_model <- df_encoded
# Extract centroid coordinates
df_model$lon <- st_coordinates(st_centroid(df_model$geometry))[,1]
df_model$lat <- st_coordinates(st_centroid(df_model$geometry))[,2]
df_model <- df_model[!is.na(df_model$lon) & !is.na(df_model$lat), ]
# glm_model <- glm(risk ~ operator_number + flowline_id + location_id + status +
#                  fluid + material + location_type + root_cause +
#                  diameter_in + length_ft + max_operating_pressure +
#                  line_age_yr + average_pop_density + Avg_Elevation,
#                  data = df_model,
#                  family = binomial)
glm_model <- glm(risk ~ diameter_in + length_ft + max_operating_pressure +
line_age_yr + average_pop_density + Avg_Elevation,
data = df_model,
family = binomial)
summary(glm_model)
# Extract residuals from logistic regression
df_model$residuals <- residuals(glm_model, type = "deviance")
# Create spatial weights matrix based on nearest neighbors of centroids
coords <- cbind(df_model$lon, df_model$lat)
nb <- knn2nb(knearneigh(coords, k = 5))  # 5 nearest neighbors
lw <- nb2listw(nb, style = "W")
# Moran’s I test for spatial autocorrelation
moran_test <- moran.test(df_model$residuals, lw)
print(moran_test)
# Fit thin-plate spline (Tps) model to capture spatial structure
# spatial process function instead of tps
spatial_model <- Tps(coords, df_model$residuals)
# Check spatial model summary
summary(spatial_model)
df_model$spatial_effect <- predict(spatial_model, coords)
glm_spatial <- glm(risk ~ diameter_in + length_ft + max_operating_pressure +
line_age_yr + average_pop_density + Avg_Elevation + spatial_effect,
data = df_model,
family = binomial)
summary(glm_spatial)
df_model$risk_pred <- predict(glm_spatial, type = "response")
df_model$risk_pred_scaled <- scales::rescale(df_model$risk_pred, to = c(0,1))
library(RColorBrewer)       # High-contrast perceptual color scales
# Define the color scale to match the image
risk_palette <- colorNumeric(
palette = rev(brewer.pal(11, "RdYlBu")),  # Reverse to match image
domain = df_model$risk_pred_scaled
)
leaflet(df_model) %>%
addProviderTiles(providers$CartoDB.Positron) %>%  # Add background map
addPolylines(
color = ~risk_palette(risk_pred_scaled),  # Use continuous color scale
weight = 3,  # Make lines thicker for visibility
opacity = 0.9  # Make colors stand out more
) %>%
addLegend(
"bottomright",
pal = risk_palette,
values = df_model$risk_pred_scaled,
title = "Predicted Risk Level",
labFormat = labelFormat(transform = function(x) round(x, 2))  # Round legend values
)
# Compare AIC values (lower AIC = better fit)
AIC(glm_model)  # Logistic regression without spatial effects
AIC(glm_spatial)  # Logistic regression with spatial effects from `fields`
knitr::opts_chunk$set(echo = TRUE)
library(sf)
library(fields)
library(pROC)
library(dplyr)
source("logisticSmoother.R")
knitr::opts_chunk$set(echo = TRUE)
library(sf)
library(fields)
library(pROC)
library(dplyr)
setwd("~/Desktop/MINES/COGCC-Risk-Analysis/IC_Scripts/2024-2025/fields_example")
source("logisticSmoother.R")
source("logisticRegressionSimple.R")
flowlines <- st_read("final_dataset.geojson")
knitr::opts_chunk$set(echo = TRUE)
library(sf)
library(fields)
library(pROC)
library(dplyr)
setwd("~/Desktop/MINES/COGCC-Risk-Analysis/IC_Scripts/2024-2025/fields_example")
source("logisticSmoother.R")
source("logisticRegressionSimple.R")
knitr::opts_chunk$set(echo = TRUE)
library(sf)
library(fields)
library(pROC)
library(dplyr)
setwd("~/Desktop/MINES/COGCC-Risk-Analysis/IC_Scripts/2024-2025/fields_example")
source("logisticSmoother.R")
source("logisticRegressionSimple.R")
setwd("~/Desktop/MINES/COGCC-Risk-Analysis/Data")
flowlines <- st_read("final_dataset.geojson")
knitr::opts_chunk$set(echo = TRUE)
library(sf)
library(fields)
library(pROC)
library(dplyr)
setwd("~/Desktop/MINES/COGCC-Risk-Analysis/IC_Scripts/2024-2025/fields_example")
source("logisticSmoother.R")
source("logisticRegressionSimple.R")
setwd("~/Desktop/MINES/COGCC-Risk-Analysis/Data")
flowlines <- st_read("final_dataset.geojson")
drop.cols <- c("unique_id","operator_number","flowline_id","location_id",
"construct_date","spill_date","root_cause")
flowlines <- flowlines[, setdiff(names(flowlines), drop.cols)]
View(flowlines)
# 1. Get all incomplete rows (with at least one NA)
incomplete_rows <- df[!complete.cases(df), ]
# 1. Get all incomplete rows (with at least one NA)
incomplete_rows <- flowlines[!complete.cases(df), ]
# 1. Get all incomplete rows (with at least one NA)
incomplete_rows <- flowlines[!complete.cases(flowlines), ]
# Remove the geometry column to get a regular data frame
flowlines_df <- st_drop_geometry(flowlines)
# Now get incomplete rows
incomplete_rows <- flowlines_df[!complete.cases(flowlines_df), ]
# Count how many are incomplete
num_incomplete <- nrow(incomplete_rows)
# Count how many have risk == 0 or 1
num_risk_0 <- sum(incomplete_rows$risk == 0, na.rm = TRUE)
num_risk_1 <- sum(incomplete_rows$risk == 1, na.rm = TRUE)
# Optional print
cat("Total incomplete rows:", num_incomplete, "\n")
cat("Incomplete rows with risk == 0:", num_risk_0, "\n")
cat("Incomplete rows with risk == 1:", num_risk_1, "\n")
# Drop geometry so we can work with a normal data frame
df <- st_drop_geometry(flowlines)
# Filter to rows with any NAs
na_rows <- df[!complete.cases(df), ]
# Print how many rows have NAs
cat("Number of rows with NAs:", nrow(na_rows), "\n")
# Print the risk values of those rows
cat("Risk values of rows with NAs:\n")
print(na_rows$risk)
df <- st_drop_geometry(flowlines)
na_rows <- df[!complete.cases(df), ]
total_na <- nrow(na_rows)
risk_0 <- sum(na_rows$risk == 0, na.rm = TRUE)
risk_1 <- sum(na_rows$risk == 1, na.rm = TRUE)
cat(total_na, risk_0, risk_1, "\n")
df <- st_drop_geometry(flowlines)
na_rows <- df[!complete.cases(df), ]
risk_0 <- sum(na_rows$risk == 0, na.rm = TRUE)
risk_1 <- sum(na_rows$risk == 1, na.rm = TRUE)
cat(risk_0, risk_1, "\n")
View(df)
df <- st_drop_geometry(flowlines)
na_rows <- df[!complete.cases(df), ]
risk_0 <- sum(na_rows$risk == 0, na.rm = TRUE)
risk_1 <- sum(na_rows$risk == 1, na.rm = TRUE)
cat("Rows with NAs and risk == 0:", risk_0, "\n")
cat("Rows with NAs and risk == 1:", risk_1, "\n")
knitr::opts_chunk$set(echo = TRUE)
library(sf)
library(fields)
library(pROC)
library(dplyr)
setwd("~/Desktop/MINES/COGCC-Risk-Analysis/IC_Scripts/2024-2025/fields_example")
source("logisticSmoother.R")
source("logisticRegressionSimple.R")
setwd("~/Desktop/MINES/COGCC-Risk-Analysis/Data")
flowlines <- st_read("final_dataset.geojson")
drop.cols <- c("unique_id","operator_number","flowline_id","location_id",
"construct_date","spill_date","root_cause")
flowlines <- flowlines[, setdiff(names(flowlines), drop.cols)]
knitr::opts_chunk$set(echo = TRUE)
library(sf)
library(fields)
library(pROC)
library(dplyr)
setwd("~/Desktop/MINES/COGCC-Risk-Analysis/IC_Scripts/2024-2025/fields_example")
source("logisticSmoother.R")
source("logisticRegressionSimple.R")
setwd("~/Desktop/MINES/COGCC-Risk-Analysis/Data")
flowlines <- st_read("final_dataset.geojson")
drop.cols <- c("unique_id","operator_number","flowline_id","location_id",
"construct_date","spill_date","root_cause")
flowlines <- flowlines[, setdiff(names(flowlines), drop.cols)]
df <- st_drop_geometry(flowlines)
na_rows <- df[!complete.cases(df), ]
risk_0 <- sum(na_rows$risk == 0, na.rm = TRUE)
risk_1 <- sum(na_rows$risk == 1, na.rm = TRUE)
cat("Rows with NAs and risk == 0:", risk_0, "\n")
cat("Rows with NAs and risk == 1:", risk_1, "\n")
df <- df[complete.cases(df), ]
31942-24196
to.factor <- c("status","flowline_action","location_type","fluid","material")
flowlines <- flowlines %>% mutate(across(all_of(to.factor), as.factor))
View(df)
# Check for duplicated values in the coords column
duplicated_coords <- duplicated(df$coords)
# View number of duplicates
sum(duplicated_coords)
# View the duplicated coordinate values
unique(df$coords[duplicated_coords])
