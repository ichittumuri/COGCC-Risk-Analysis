---
title: "spatial process"
output: pdf_document
date: "2025-03-03"
---

# Setup

```{r setup, include=FALSE}
# New cell= command, option, I or alt, windows symbol, I 
# Run = windows symbol, enter 

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_knit$set(root.dir = "/Users/ichittumuri/Desktop/MINES/COGCC-Risk-Analysis/Data")
```

```{r}
# Load required libraries
library(sf)        # For spatial data handling
library(spBayes)   # For Bayesian spatial models
library(MASS)      # For generalized linear models
library(spdep)     # For spatial correlation structures
library(ggplot2)   # For visualization
library(dplyr)     # For data wrangling
# library(fmesher)
# library(INLA)      # Optional faster spatial GLM alternative
```

# Import Dataset 

```{r cars}
df <- st_read("flowlines_pop_dems.geojson")
```
```{r}
colSums(is.na(df))  # Check missing data per column
```
```{r}
# Filter rows where root_cause is NA and count by risk values
table(df$risk[is.na(df$root_cause)])
```

```{r}
unique(df$root_cause)
```
```{r}
df$root_cause[is.na(df$root_cause)] <- "None"
```

```{r}
str(df)
```

```{r}
unique(df$risk)
```

```{r}
colnames(df)
```

```{r}
df <- df %>%
  select(-Max_Elevation, -Min_Elevation, -operator_number, -flowline_id, -location_id, -root_cause)
```

```{r}
# Duplicate data set 
df_encoded <- df
```

```{r}
# One-hot encoding
df_encoded$status <- as.factor(df$status)
df_encoded$fluid <- as.factor(df$fluid)
df_encoded$material <- as.factor(df$material)
df_encoded$location_type <- as.factor(df$location_type)
```

```{r}
str(df_encoded)
```

```{r}
colSums(is.na(df_encoded))  # Check missing data per column
```

# EDA

```{r}
library(leaflet)

leaflet(df_encoded) %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addPolylines(color = ~ifelse(risk == 1, "red", "blue"), weight = 2, opacity = 0.7) %>%
  addLegend("bottomright", colors = c("blue", "red"), labels = c("No Spill", "Spill"),
            title = "Risk Level")
```

# Preprocessing 

```{r}
# Use the factor-encoded dataset
df_clean <- df_encoded

# Extract centroid coordinates from the geometry column
df_clean$lon <- st_coordinates(st_centroid(df_encoded$geometry))[,1]
df_clean$lat <- st_coordinates(st_centroid(df_encoded$geometry))[,2]
df_clean <- df_clean[!is.na(df_clean$lon) & !is.na(df_clean$lat), ]

# Check binary response variable
table(df_clean$risk)  # Ensure it's 0/1

library(sf)

df_final <- df_clean %>%
  # st_drop_geometry() %>%
  select(risk, status, fluid, material, location_type, diameter_in, length_ft, 
         max_operating_pressure, line_age_yr, average_pop_density, Avg_Elevation, lon, lat)

colnames(df_final)
```

#Reg GlM model

```{r}
glm_model <- glm(risk ~ status + fluid + material + location_type  +
                   diameter_in  + max_operating_pressure + line_age_yr + 
                   average_pop_density + Avg_Elevation, 
                 data = df_final, 
                 family = binomial)

summary(glm_model)  # Check model output
```

# Check spatial correlation

```{r}
# Extract residuals from logistic regression
df_final$residuals <- residuals(glm_model, type = "deviance")

# Create spatial weights matrix based on nearest neighbors
coords <- cbind(df_final$lon, df_final$lat)
nb <- knn2nb(knearneigh(coords, k = 5))  # 5 nearest neighbors
lw <- nb2listw(nb, style = "W")

# Moran’s I test for spatial autocorrelation
moran_test <- moran.test(df_final$residuals, lw)
print(moran_test)
```

# Feature Selection 

## AIC, stepwise model
```{r}
# Fit a full model using all predictors (ensure categorical variables are handled appropriately)
full_model <- glm(risk ~ status + fluid + material + location_type +
                    diameter_in + length_ft + max_operating_pressure + line_age_yr + 
                    average_pop_density + Avg_Elevation, 
                  data = df_final, family = binomial)

# Fit a null model (intercept only)
null_model <- glm(risk ~ 1, data = df_final, family = binomial)

# Perform stepwise selection based on AIC
stepwise_model <- step(null_model, scope = list(lower = null_model, upper = full_model, k=log(16121)), direction = "both")
summary(stepwise_model)
```


```{r}
# after you’ve built your data.frame for the stack, do:
dat  <- inla.stack.data(stack)
classes <- sapply(dat, class)
print(classes)
```



```{r}
library(INLA)

df <- df_final


# 1) Keep status, fluid, material, location_type as factors
df$status        <- factor(df$status)
df$fluid         <- factor(df$fluid)
df$material      <- factor(df$material)
df$location_type <- factor(df$location_type)

# 2) Build stack with just your data.frame (no grep):
stack <- inla.stack(
  data = list(y = df$risk),
  A    = list(1, A),
  effects = list(
    data.frame(
      intercept = 1,
      status        = df$status,
      fluid         = df$fluid,
      material      = df$material,
      location_type = df$location_type,
      diameter_in   = df$diameter_in,
      length_ft     = df$length_ft,
      max_operating_pressure = df$max_operating_pressure,
      line_age_yr   = df$line_age_yr,
      average_pop_density = df$average_pop_density,
      Avg_Elevation = df$Avg_Elevation
    ),
    spatial = s.index
  ),
  tag = "est"
)

# 3) Formula lets INLA make the dummies:
formula <- y ~ -1 + intercept +
     status + fluid + material + location_type +
     diameter_in + length_ft + max_operating_pressure +
     line_age_yr + average_pop_density + Avg_Elevation +
     f(spatial, model = spde)

# 4) Fit as before
res <- inla(
  formula,
  family = "binomial",
  data   = inla.stack.data(stack),
  control.predictor = list(A = inla.stack.A(stack), compute = TRUE),
  control.fixed     = list(mean = list(intercept = qlogis(mean(df$risk))),
                           prec = list(intercept = 0.01)),
  control.compute   = list(dic = TRUE, waic = TRUE)
)
```



```{r}
df <- df_final

# install.packages("INLA", repos="https://inla.r-inla-download.org/R/stable")
library(INLA)

# 0. If df is an sf, drop the geometry so we don't carry list-columns around
if ("sf" %in% class(df)) {
  df <- sf::st_drop_geometry(df)
}

# 1. Robustly coerce lon/lat into numeric
coord_vars <- c("lon","lat")
bad_coords <- coord_vars[ sapply(coord_vars, function(v) !is.numeric(df[[v]])) ]
if (length(bad_coords)) {
  message("Coercing these coordinate cols to numeric: ", paste(bad_coords, collapse=", "))
  for (v in bad_coords) {
    x <- df[[v]]
    if (is.list(x))     x <- unlist(x)
    if (is.factor(x))   x <- as.character(x)
    if (is.character(x)) x <- as.numeric(x)
    df[[v]] <- as.numeric(x)
  }
}
# sanity check
stopifnot(all(sapply(coord_vars, function(v) is.numeric(df[[v]]))),
          !any(is.na(df[coord_vars])))

# 2. Robustly coerce continuous predictors into numeric
cont_vars <- c(
  "diameter_in","length_ft","max_operating_pressure",
  "line_age_yr","average_pop_density","Avg_Elevation"
)
bad_cont <- cont_vars[ sapply(cont_vars, function(v) !is.numeric(df[[v]])) ]
if (length(bad_cont)) {
  message("Coercing these cont cols to numeric: ", paste(bad_cont, collapse=", "))
  for (v in bad_cont) {
    x <- df[[v]]
    if (is.list(x))     x <- unlist(x)
    if (is.factor(x))   x <- as.character(x)
    if (is.character(x)) x <- as.numeric(x)
    df[[v]] <- as.numeric(x)
  }
}
# sanity check
stopifnot(all(sapply(cont_vars, function(v) is.numeric(df[[v]]))),
          !any(is.na(df[cont_vars])))

# 3. Factorize categorical predictors so INLA will dummy-code them
df$status        <- factor(df$status)
df$fluid         <- factor(df$fluid)
df$material      <- factor(df$material)
df$location_type <- factor(df$location_type)

# 4. Build the SPDE mesh & PC-prior
coords <- as.matrix(df[, coord_vars])

mesh <- inla.mesh.2d(
  loc      = coords,
  max.edge = c(0.05, 0.2),   # adjust to your spatial scale
  cutoff   = 0.01
)

spde <- inla.spde2.pcmatern(
  mesh         = mesh,
  alpha        = 2,
  prior.range  = c(0.1, 0.01),  # P(range < 0.1) = 0.01
  prior.sigma  = c(1,   0.01)   # P(sigma > 1) = 0.01
)

A       <- inla.spde.make.A(mesh, loc = coords)
s.index <- inla.spde.make.index("spatial", n.spde = spde$n.spde)

# 5. Stack data (fixed effects + spatial)
stk <- inla.stack(
  data    = list(y = df$risk),
  A       = list(1, A),
  effects = list(
    data.frame(
      intercept              = 1,
      status                 = df$status,
      fluid                  = df$fluid,
      material               = df$material,
      location_type          = df$location_type,
      diameter_in            = df$diameter_in,
      length_ft              = df$length_ft,
      max_operating_pressure = df$max_operating_pressure,
      line_age_yr            = df$line_age_yr,
      average_pop_density    = df$average_pop_density,
      Avg_Elevation          = df$Avg_Elevation
    ),
    spatial = s.index
  ),
  tag = "est"
)

# 6. Specify the formula
formula <- y ~ -1 + intercept +
           status + fluid + material + location_type +
           diameter_in + length_ft + max_operating_pressure +
           line_age_yr + average_pop_density + Avg_Elevation +
           f(spatial, model = spde)

# 7. Fit the INLA model with an informative intercept prior
p0    <- mean(df$risk)    # empirical event rate
beta0 <- qlogis(p0)

res <- inla(
  formula,
  family            = "binomial",
  data              = inla.stack.data(stk),
  control.predictor = list(A = inla.stack.A(stk), compute = TRUE),
  control.fixed     = list(
    mean = list(intercept = beta0),
    prec = list(intercept = 0.01)
  ),
  control.compute   = list(dic = TRUE, waic = TRUE)
)

# 8. Examine results
print(summary(res))


```























# TPS

```{r}
library(fields)

# Fit thin-plate spline (Tps) model to capture spatial structure
# spatial process function instead of tps 
spatial_model <- Tps(coords, df_final$residuals)

# Check spatial model summary
summary(spatial_model)

df_final$spatial_effect <- predict(spatial_model, coords)

glm_spatial <- glm(risk ~ status + fluid + material + location_type  +
                   diameter_in  + max_operating_pressure + line_age_yr + 
                   average_pop_density + Avg_Elevation + spatial_effect, 
                   data = df_final, 
                   family = binomial)

summary(glm_spatial)

df_final$risk_pred <- predict(glm_spatial, type = "response")
df_final$risk_pred_scaled <- scales::rescale(df_final$risk_pred, to = c(0,1))

library(RColorBrewer)       # High-contrast perceptual color scales

# Define the color scale to match the image
risk_palette <- colorNumeric(
  palette = rev(brewer.pal(11, "RdYlBu")),  # Reverse to match image
  domain = df_final$risk_pred_scaled
)

leaflet(df_final) %>%ß
  addProviderTiles(providers$CartoDB.Positron) %>%  # Add background map
  addPolylines(
    color = ~risk_palette(risk_pred_scaled),  # Use continuous color scale
    weight = 3,  # Make lines thicker for visibility
    opacity = 0.9  # Make colors stand out more
  ) %>%
  addLegend(
    "bottomright", 
    pal = risk_palette, 
    values = df_final$risk_pred_scaled, 
    title = "Predicted Risk Level",
    labFormat = labelFormat(transform = function(x) round(x, 2))  # Round legend values
  )
```

```{r}
# Compare AIC values (lower AIC = better fit)
AIC(glm_model)  # Logistic regression without spatial effects
AIC(glm_spatial)  # Logistic regression with spatial effects from `fields`
```


# INLA

```{r}
library(INLA)

# 1. Build a mesh over your points
locs <- cbind(df_final$lon, df_final$lat)
mesh <- inla.mesh.2d(loc = locs, max.edge = c(0.1, 0.5), cutoff = 0.01)

# 2. Define the Matern SPDE
spde <- inla.spde2.matern(mesh = mesh, alpha = 2)

# 3. Create projector matrix
A <- inla.spde.make.A(mesh = mesh, loc = locs)

# 4. Stack data for estimation
stack <- inla.stack(
  data = list(y = df_final$risk),
  A    = list(1, A),
  effects = list(
    data.frame(
      intercept = 1,
      status = df_final$status,
      fluid  = df_final$fluid,
      material = df_final$material,
      location_type = df_final$location_type,
      diameter_in = df_final$diameter_in,
      length_ft  = df_final$length_ft,
      max_operating_pressure = df_final$max_operating_pressure,
      line_age_yr = df_final$line_age_yr,
      average_pop_density = df_final$average_pop_density,
      Avg_Elevation      = df_final$Avg_Elevation
    ),
    s = inla.spde.make.index("s", n.spde = spde$n.spde)
  ),
  tag = "est"
)

# 5. Define formula with spatial random effect f(s, ...)
formula <- y ~ -1 + intercept
formula <- update(formula,
  . ~ . + status + fluid + material + location_type
  + diameter_in + length_ft + max_operating_pressure + line_age_yr
  + average_pop_density + Avg_Elevation
  + f(s, model = spde)
)

# 6. Fit the model
res_inla <- inla(
  formula,
  family = "binomial",
  data   = inla.stack.data(stack),
  control.predictor = list(A = inla.stack.A(stack), compute = TRUE),
  control.family    = list(link = "logit"),
  control.compute   = list(
    config = FALSE,   # do NOT write the full posterior config
    dic    = TRUE,
    waic   = TRUE
  ),
  control.inla      = list(strategy = "laplace"),
  verbose = FALSE
)

# 7. Inspect results
summary(res_inla)

# 8. Extract fitted risk
idx_est <- inla.stack.index(stack, "est")$data
risk_est <- res_inla$summary.fitted.values[idx_est, "mean"]

# Addressing imbalance
# • INLA allows case‐control offsets: create weights vector and pass via 'scale' in control.family.
# • Or replicate minority‐class rows in your stack to upweight them.
```


```{r}
library(leaflet)

pal <- colorNumeric("viridis", domain = df_final$pred_risk)

leaflet(df_final) %>%
  addProviderTiles("CartoDB.Positron") %>%
  addCircleMarkers(
    ~lon, ~lat,
    radius = 4,
    color  = ~pal(pred_risk),
    stroke = FALSE, fillOpacity = 0.8
  ) %>%
  addLegend(
    "bottomright",
    pal   = pal,
    values = ~pred_risk,
    title  = "Predicted Risk"
  )
```
















# fields

```{r}
library(fields)

# 1. Subset to complete cases again
keep <- complete.cases(
  df_final[, c("risk","status","fluid","material","location_type",
               "diameter_in","length_ft","max_operating_pressure",
               "line_age_yr","average_pop_density","Avg_Elevation",
               "lon","lat")]
)
df2 <- df_final[keep, ]

# 2. Response + coords
y      <- df2$risk
coords <- as.matrix(df2[, c("lon","lat")])

# 3. Build a *full-rank* Z:
#    - Use “-1” so model.matrix does NOT include its own intercept
#    - Only one dummy per factor level set
Z_factors <- model.matrix(
  ~ status + fluid + material + location_type - 1,
  data = df2
)
Z_numeric <- as.matrix(df2[, c(
  "diameter_in","length_ft","max_operating_pressure",
  "line_age_yr","average_pop_density","Avg_Elevation"
)])
Zmat <- cbind(Z_factors, Z_numeric)

# 4. Check rank:
stopifnot(qr(Zmat)$rank == ncol(Zmat))  # should pass; otherwise drop colinear columns

# 5. Fit
fit_fields <- Krig(
  x      = coords,
  Y      = y,
  Z      = Zmat,
  m      = 2,
  lambda = NULL
)
summary(fit_fields)
```


# spBayes

```{r}
# install and load sf (and spBayes)
if (!require(sf)) install.packages("sf")
library(sf)
library(spBayes)

# 1. Convert to sf and project to UTM Zone 13N, then extract coords in km
pts_ll   <- st_as_sf(df_final, coords = c("lon", "lat"), crs = 4326)
pts_utm  <- st_transform(pts_ll, crs = 32613)
coords_km <- st_coordinates(pts_utm) / 1000

# 2. Build design matrix and response
X <- model.matrix(
  ~ status + fluid + material + location_type
    + diameter_in + length_ft + max_operating_pressure
    + line_age_yr + average_pop_density + Avg_Elevation,
  data = df_final
)
y <- df_final$risk
p <- ncol(X)
n <- nrow(df_final)

# 3. Compute sensible φ bounds from pairwise distances
dmat       <- as.matrix(dist(coords_km))
dmax       <- max(dmat)
dmin       <- min(dmat[dmat > 0])
phi_lower  <- 3 / dmax
phi_upper  <- 3 / dmin

# 4. Specify priors
priors <- list(
  beta.Norm   = list(mean = rep(0, p), var = diag(1e6, p)),
  phi.Unif    = c(phi_lower, phi_upper),
  sigma.sq.IG = c(2, 1)
)

# 5. Starting values (include spatial effects w)
starting <- list(
  beta     = rep(0, p),
  w        = rep(0, n),
  phi      = mean(c(phi_lower, phi_upper)),
  sigma.sq = 1
)

# 6. Tuning parameters (must include w)
tuning <- list(
  beta     = rep(0.1, p),
  w        = rep(0.01, n),
  phi      = (phi_upper - phi_lower) * 0.1,
  sigma.sq = 0.1
)

# 7. Fit the spatial GLM
bayes_spglm <- spGLM(
  y ~ X - 1,
  coords    = coords_km,
  family    = "binomial",
  cov.model = "exponential",
  priors    = priors,
  starting  = starting,
  tuning    = tuning,
  n.samples = 20000,
  verbose   = TRUE
)

# 8. Summarize posteriors
summary(bayes_spglm$p.beta.samples)
summary(bayes_spglm$p.theta.samples)
```








#  logit offset

```{r}
library(INLA)
library(sf)
library(dplyr)

# Step 1: Geometry + Coordinates
coords <- st_coordinates(st_centroid(df_model$geometry))
df_model$X <- coords[, 1]
df_model$Y <- coords[, 2]

# Step 2: Spatial Mesh and SPDE
mesh <- inla.mesh.2d(loc = coords, max.edge = c(0.5, 2), cutoff = 0.1)
spde <- inla.spde2.pcmatern(
  mesh,
  prior.range = c(10, 0.5),   # P(range < 10) = 0.5
  prior.sigma = c(1, 0.01)    # P(sigma > 1) = 0.01
)
A <- inla.spde.make.A(mesh = mesh, loc = coords)

# Step 3: Scale numeric predictors
df_model$average_pop_density <- scale(df_model$average_pop_density)
df_model$line_age_yr <- scale(df_model$line_age_yr)
df_model$max_operating_pressure <- scale(df_model$max_operating_pressure)
df_model$diameter_in <- scale(df_model$diameter_in)
df_model$Avg_Elevation <- scale(df_model$Avg_Elevation)
df_model$length_ft <- scale(df_model$length_ft)

# Step 4: Compute logit offset for imbalance correction
spill_rate <- mean(df_model$risk)
df_model$offset_term <- log(spill_rate / (1 - spill_rate))

# Step 5: Build INLA stack
df_model$Intercept <- 1

stack <- inla.stack(
  data = list(risk = df_model$risk, offset_term = df_model$offset_term),  # <-- here!
  A = list(1, A),
  effects = list(
    data.frame(
      Intercept = df_model$Intercept,
      length_ft = df_model$length_ft,
      material = df_model$material,
      fluid = df_model$fluid,
      location_type = df_model$location_type,
      status = df_model$status,
      average_pop_density = df_model$average_pop_density,
      line_age_yr = df_model$line_age_yr,
      max_operating_pressure = df_model$max_operating_pressure,
      diameter_in = df_model$diameter_in,
      Avg_Elevation = df_model$Avg_Elevation
    ),
    s = 1:spde$n.spde
  ),
  tag = "est"
)

formula <- risk ~ 0 + Intercept + length_ft + material + fluid + location_type +
  status + average_pop_density + line_age_yr + max_operating_pressure +
  diameter_in + Avg_Elevation + f(s, model = spde) + offset(offset_term)

result_inla <- inla(
  formula,
  family = "binomial",
  data = inla.stack.data(stack),  # includes offset_term
  control.predictor = list(A = inla.stack.A(stack), compute = TRUE),
  control.compute = list(dic = TRUE, waic = TRUE, cpo = TRUE)
)

```

```{r}
# Compute mean risk in the full dataset
spill_rate <- mean(df_model$risk); spill_rate

# Compute the logit offset
offset_logit <- log(spill_rate / (1 - spill_rate)); offset_logit
```


```{r}
library(sf)
library(INLA)

# Ensure geometry is in the right format
coords <- st_coordinates(st_centroid(df_model$geometry))
df_model$X <- coords[, 1]
df_model$Y <- coords[, 2]

# Create spatial mesh from coordinates
mesh <- inla.mesh.2d(loc = coords, max.edge = c(0.5, 2), cutoff = 0.1)

# Define SPDE model
spde <- inla.spde2.matern(mesh)
A <- inla.spde.make.A(mesh = mesh, loc = coords)

# Choose some predictors for now (e.g., fluid and pop density)
df_model$Intercept <- 1

stack <- inla.stack(
  data = list(risk = df_model$risk),
  A = list(1, A),
  effects = list(
    data.frame(Intercept = df_model$Intercept,
               fluid = df_model$fluid,
               average_pop_density = df_model$average_pop_density),
    s = 1:spde$n.spde
  ),
  tag = "est"
)

# Fit model
formula <- risk ~ 0 + Intercept + fluid + average_pop_density + f(s, model = spde)

result_inla <- inla(
  formula,
  family = "binomial",
  data = inla.stack.data(stack),
  control.predictor = list(A = inla.stack.A(stack), compute = TRUE),
  control.compute = list(dic = TRUE, waic = TRUE)
)

summary(result_inla)
```





























# SpBayes & Fields Models & Uncertainty Quantifications

Select a threshold for the points that are too close to each other.

spill - no spill, discard no spill
spill - spill, avg
no spill - no spill, avg

```{r}
library(spBayes)

coords <- cbind(df_model$lon, df_model$lat)

# Define starting values with added 'nu' parameter
starting <- list(
  beta = rep(0, 7),  
  phi = 1 / max(dist(coords)),  # Adjusted phi for stability
  sigma.sq = 2,  
  tau.sq = 1,  
  w = rep(0, nrow(coords)),  
  nu = 1.5  # Added nu for Matérn covariance model
)

# Define tuning parameters (including nu)
tuning <- list(
  beta = rep(0.2, 7),  
  phi = 0.5,  
  sigma.sq = 0.5,  
  tau.sq = 0.5,  
  w = rep(0.5, nrow(coords)),  
  nu = 0.1  # Tuning for nu
)

# Define priors
priors <- list(
  beta.norm = list(rep(0, 7), diag(100, 7)),  
  phi.unif = c(0.1, 10 / max(dist(coords))),  
  sigma.sq.ig = c(2, 2),  
  tau.sq.ig = c(2, 2),
  nu.unif = c(0.5, 2)  # Prior range for nu
)

# Run Bayesian spatial GLM
spatial_model <- spGLM(risk ~ diameter_in + length_ft + max_operating_pressure + 
                         line_age_yr + average_pop_density + Avg_Elevation, 
                       coords = coords, 
                       data = df_model,
                       family = "binomial",
                       starting = starting,
                       tuning = tuning,  
                       priors = priors,
                       cov.model = "exponential",  # Matérn covariance model requires nu
                       n.samples = 2000)  

# Check model summary
summary(spatial_model)
```


```{r}
install.packages("INLA", repos="https://inla.r-inla-download.org/R/testing")
```

```{r}
library(INLA)

# Define the model formula
formula <- risk ~ diameter_in + length_ft + max_operating_pressure +
  line_age_yr + average_pop_density + Avg_Elevation + f(lon, lat, model="spde")

# Fit spatial GLM using INLA
spatial_inla <- inla(formula, family="binomial", data=df_model, 
                     control.predictor=list(compute=TRUE))

summary(spatial_inla)  # Check results
```

```{r}
# Compare AIC values
AIC(glm_model)  # Standard logistic regression
AIC(spatial_model)  # Bayesian spatial logistic regression
```

```{r}
head(df_network)
```

```{r}
library(sfnetworks)
library(dplyr)

# Convert to an sf object first, ensuring attributes are retained
df_model_sf <- st_as_sf(df_model, coords = c("lon", "lat"), crs = 4326)

# Convert to a network while preserving attributes
df_network <- as_sfnetwork(df_model_sf, directed = FALSE)

# Extract node attributes and confirm risk is present
df_nodes <- df_network %>% activate(nodes) %>% as_tibble()

df_nodes <- df_nodes %>%
  st_join(df_model %>% select(lon, lat, risk))

if (!"risk" %in% colnames(df_nodes)) {
    stop("Variable 'risk' still missing after conversion.")
}

# Fit a spatial autoregressive model along the network
network_model <- glm(risk ~ diameter_in + length_ft + max_operating_pressure + 
                       line_age_yr + average_pop_density + Avg_Elevation,
                     data = df_nodes, family = binomial)

summary(network_model)
```

```{r}
print(colnames(df_model))
```

```{r}
library(sf)
# library(rgeos)
library(ggplot2)

# Ensure data is in sf format
df_model <- df_encoded

# Compute pairwise Hausdorff distance between flowlines
# dist_matrix <- gDistance(st_geometry(df_model), byid = TRUE)

# Compute distance matrix (Euclidean distance by default)
dist_matrix <- st_distance(df_model)

# Convert to data frame for visualization
dist_df <- as.data.frame(as.table(dist_matrix))

# Rename columns for clarity
colnames(dist_df) <- c("Flowline_1", "Flowline_2", "Hausdorff_Distance")

# Show the first few rows
head(dist_df)
```

```{r}
head(dist_df)
```


```{r}
ggplot(dist_df, aes(x = Flowline_1, y = Flowline_2, fill = Hausdorff_Distance)) +
  geom_tile() +
  scale_fill_viridis_c(option = "plasma") +
  theme_minimal() +
  labs(title = "Spatial Dependency Between Flowlines (Hausdorff Distance)",
       x = "Flowline ID", y = "Flowline ID")
```

```{r}
library(sf)
library(dplyr)

# Read original MULTILINESTRING dataset
df_original <- st_read("matched_flowlines.geojson")

# Ensure they have the same CRS (Coordinate Reference System)
df_encoded <- st_transform(df_encoded, st_crs(df_original))
```

```{r}
head(df_encoded)
```

```{r}
# Use the factor-encoded dataset
df_model <- df_encoded

df_original_geom <- df_original %>% select(geometry)  # Keep only the geometry

# Spatial join: assign each 50m segment to its parent MULTILINESTRING
df_model <- st_join(df_model, df_original_geom, join = st_within, suffix = c("", "_parent"))

# Count how many segments belong to each original flowline
segment_counts <- df_model %>%
  group_by(geometry) %>%  # Group by the original MULTILINESTRING geometry
  summarise(num_segments = n())

# View number of segments per original flowline
print(segment_counts)
```

```{r}
head(df_model)
```

```{r}
# Use the factor-encoded dataset
df_model <- df_encoded

# Spatial join: assign each 50m segment to its parent MULTILINESTRING
df_model <- st_join(df_model, df_original, join = st_within)

# Count how many segments belong to each original flowline
segment_counts <- df_model %>%
  group_by(geometry) %>%  # Group by the original MULTILINESTRING geometry
  summarise(num_segments = n())

# View number of segments per original flowline
print(segment_counts)
```


```{r}
head(df_model)
```

Your dataset is in NAD83 (EPSG:4269), which uses degrees (longitude/latitude) as units. However, dnearneigh() expects distances in meters, so you need to reproject the data to a projected CRS before computing neighbors.

```{r}
library(sf)

# Reproject to a UTM Zone or an Equal Area projection
df_model <- st_transform(df_encoded, crs = 5070)  # NAD83 Albers Equal Area (meters)

# Check the new CRS
st_crs(df_model)
```

1. Using sf Objects Directly in Spatial Models
- Some spatial models (like geostatistical models) require point data, but others (like spatial autoregressive models) can directly use LINESTRING geometries.
- You can avoid converting to centroids and instead use the full flowline data by:
  - Constructing spatial neighbors (e.g., flowlines that are spatially connected).
  - Using a network-based spatial correlation model.
  
2. Adjusting Spatial Weight Matrices for Line Data
- Instead of using point-based nearest neighbors, you can:
- Use contiguity-based neighbors, where flowlines that share a boundary are considered neighbors.
- Construct a network distance matrix, where spatial relationships are based on shared LINESTRING intersections.
 
```{r}
library(spdep)

# Ensure valid geometries before computing centroids
df_model$geometry <- st_make_valid(df_model$geometry)

# Compute centroids in meters
df_model$centroid <- st_centroid(df_model$geometry)

# Define a reasonable distance threshold (e.g., 100 meters)
distance_threshold <- 100  

# Compute distance-based neighbors
distance_nb <- dnearneigh(df_model$centroid, d1 = 0, d2 = distance_threshold)

# Check results
summary(distance_nb)
```


```{r}
# Assuming df_original contains the original MULTILINESTRING geometries
df_original <- st_read("matched_flowlines.geojson")

# Check how many segments exist per original line
df_original %>%
  group_by() %>%
  summarise(n_segments = n())
```

```{r}
# Use the factor-encoded dataset
df_model <- df_encoded

# Extract centroid coordinates from the geometry column
df_encoded$lon <- st_coordinates(st_centroid(df$geometry))[,1]
df_encoded$lat <- st_coordinates(st_centroid(df$geometry))[,2]

# Check binary response variable
table(df_model$risk)  # Ensure it's 0/1

# Select relevant predictors including the factor variables
df_model <- df_model %>%
  select(risk, operator_number, flowline_id, location_id, status,
         fluid,material, location_type, root_cause, diameter_in,
         length_ft, max_operating_pressure, line_age_yr,
         average_pop_density, Avg_Elevation, lon, lat)
```

```{r}
glm_model <- glm(risk ~ operator_number + flowline_id + location_id +
                   status + fluid + material + location_type +
                   root_cause + diameter_in + length_ft +
                   max_operating_pressure + line_age_yr +
                   average_pop_density + Avg_Elevation, 
                 data = df_model, 
                 family = binomial)

summary(glm_model)  # Check model output
```
